{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from appworld import AppWorld, load_task_ids\n",
    "import re\n",
    "from appworld.task import Task\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.pipelines import pipeline\n",
    "from jinja2 import Template\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass \n",
    "import gc\n",
    "import tempfile\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! huggingface-cli login --token os.getenv(\"HF_TOKEN\")\n",
    "! appworld install\n",
    "! appworld download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts are modified from AppWorld's prompts\n",
    "ONE_SHOT_PROMPT = \"\"\"\n",
    "USER:\n",
    "I am your supervisor and you are a super intelligent AI Assistant whose job is to achieve my day-to-day tasks completely autonomously.\n",
    "\n",
    "To do this, you will need to interact with app/s (e.g., spotify, venmo, etc) using their associated APIs on my behalf. For this you will undertake a *multi-step conversation* using a python REPL environment. That is, you will write the python code and the environment will execute it and show you the result, based on which, you will write python code for the next step and so on, until you've achieved the goal. This environment will let you interact with app/s using their associated APIs on my behalf.\n",
    "\n",
    "IMPORTANT CODE GENERATION RULES:\n",
    "1. Each code block must be completely self-contained - variables and functions from previous blocks are NOT available\n",
    "2. DO NOT reference or try to use variables or functions defined in previous code blocks\n",
    "3. Only generate valid Python code and comments\n",
    "4. DO NOT include conversation markers (USER:, ASSISTANT:) or any other text\n",
    "5. If you need data from a previous step, you must regenerate it or fetch it again\n",
    "6. Each code block starts fresh with no access to previous variables or functions\n",
    "\n",
    "\n",
    "Here are three key APIs that you need to know to get more information\n",
    "\n",
    "# To get a list of apps that are available to you.\n",
    "print(apis.api_docs.show_app_descriptions())\n",
    "\n",
    "# To get the list of apis under any app listed above, e.g. supervisor\n",
    "print(apis.api_docs.show_api_descriptions(app_name='supervisor'))\n",
    "\n",
    "# To get the specification of a particular api, e.g. supervisor app's show_account_passwords\n",
    "print(apis.api_docs.show_api_doc(app_name='supervisor', api_name='show_account_passwords'))\n",
    "\n",
    "Each code execution will produce an output that you can use in subsequent calls. Using these PROVIDED APIs, you can now generate code, that the environment will execute, to solve the task.\n",
    "\n",
    "For example, consider the task:\n",
    "\n",
    "My name is: {{ supervisor.first_name }} {{ supervisor.last_name }}. My personal email is {{ supervisor.email }} and phone number is {{ supervisor.phone_number }}.\n",
    "\n",
    "Task:\n",
    "\n",
    "What is the password for my Spotify account?\n",
    "\n",
    "ASSISTANT:\n",
    "# Okay. Lets first find which apps are available to get the password by looking at the app descriptions.\n",
    "print(apis.api_docs.show_app_descriptions())\n",
    "\n",
    "USER:\n",
    "[\n",
    "  {\n",
    "    \"name\": \"api_docs\",\n",
    "    \"description\": \"An app to search and explore API documentation.\"\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"supervisor\",\n",
    "    \"description\": \"An app to access supervisor's personal information, account credentials, addresses, payment cards, and manage the assigned task.\"\n",
    "  },\n",
    "  ...\n",
    "  {\n",
    "    \"name\": \"spotify\",\n",
    "    \"description\": \"A music streaming app to stream songs and manage song, album and playlist libraries.\"\n",
    "  },\n",
    "  {\n",
    "    \"name\": \"venmo\",\n",
    "    \"description\": \"A social payment app to send, receive and request money to and from others.\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "\n",
    "\n",
    "ASSISTANT:\n",
    "# Looks like the supervisor app could help me with that. Lets see what apis are available under this app.\n",
    "print(apis.api_docs.show_api_descriptions(app_name='supervisor'))\n",
    "\n",
    "\n",
    "USER:\n",
    "[\n",
    "  ...\n",
    "  \"show_account_passwords : Show your supervisor's account passwords.\"\n",
    "  ...\n",
    "]\n",
    "\n",
    "\n",
    "ASSISTANT:\n",
    "# I can use `show_account_passwords` to get the passwords. Let me see its detailed specification to understand its arguments and output structure.\n",
    "print(apis.api_docs.show_api_doc(app_name='supervisor', api_name='show_account_passwords'))\n",
    "\n",
    "USER:\n",
    "{\n",
    "  'app_name': 'supervisor',\n",
    "  'api_name': 'show_account_passwords',\n",
    "  'path': '/account_passwords',\n",
    "  'method': 'GET',\n",
    "  'description': \"Show your supervisor's app account passwords.\",\n",
    "  'parameters': [],\n",
    "  'response_schemas': {\n",
    "    'success': [{'account_name': 'string', 'password': 'string'}],\n",
    "    'failure': {'message': 'string'}\n",
    "  }\n",
    "}\n",
    "\n",
    "ASSISTANT:\n",
    "# Okay, it requires no arguments. So I can just call it directly.\n",
    "print(apis.supervisor.show_account_passwords())\n",
    "\n",
    "USER:\n",
    "[\n",
    "  {\n",
    "    \"account_name\": \"spotify\",\n",
    "    \"password\": \"dummy_spotify_pass\"\n",
    "  },\n",
    "  {\n",
    "    \"account_name\": \"file_system\",\n",
    "    \"password\": \"dummy_fs_pass\"\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "\n",
    "\n",
    "ASSISTANT:\n",
    "# So the Spotify password is an entry in the `passwords` list with the account_name=spotify.\n",
    "spotify_password = [account_password[\"account_name\"] == \"spotify\" for account_password in passwords][0][\"password\"]\n",
    "print(spotify_password)\n",
    "\n",
    "\n",
    "USER:\n",
    "dummy_spotify_pass\n",
    "\n",
    "ASSISTANT:\n",
    "# When the task is completed, I need to call apis.supervisor.complete_task(). If there is an answer, I need to pass it as an argument `answer`. I will pass the spotify_password as an answer.\n",
    "apis.supervisor.complete_task(answer=spotify_password)\n",
    "\n",
    "\n",
    "USER:\n",
    "Marked the active task complete.\n",
    "\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "USER:\n",
    "**Key instructions and disclaimers**:\n",
    "\n",
    "1. The email addresses, access tokens and variables (e.g. spotify_password) in the example above were only for demonstration. Obtain the correct information by calling relevant APIs yourself.\n",
    "2. Only generate valid code blocks, i.e., do not put them in ```...``` or add any extra formatting. Any thoughts should be put as code comments.\n",
    "3. You can use the variables from the previous code blocks in the subsequent code blocks.\n",
    "4. Write small chunks of code and only one chunk of code in every step. Make sure everything is working correctly before making any irreversible change.\n",
    "5. The provided Python environment has access to its standard library. But modules and functions that have a risk of affecting the underlying OS, file system or process are disabled. You will get an error if do call them.\n",
    "6. Any reference to a file system in the task instructions means the file system *app*, operable via given APIs, and not the actual file system the code is running on. So do not write code making calls to os-level modules and functions.\n",
    "7. To interact with apps, only use the provided APIs, and not the corresponding Python packages. E.g., do NOT use `spotipy` for Spotify. Remember, the environment only has the standard library.\n",
    "8. The provided API documentation has both the input arguments and the output JSON schemas. All calls to APIs and parsing its outputs must be as per this documentation.\n",
    "9. For APIs that return results in \"pages\", make sure to consider all pages.\n",
    "10. To obtain current data or time, use Python functions like `datetime.now()` or obtain it from the phone app. Do not rely on your existing knowledge of what the current date or time is.\n",
    "11. For all temporal requests, use proper time boundaries, e.g., if I ask for something that happened yesterday, make sure to consider the time between 00:00:00 and 23:59:59. All requests are concerning a single, default (no) time zone.\n",
    "12. Any reference to my friends, family or any other person or relation refers to the people in my phone's contacts list.\n",
    "13. All my personal information, and information about my app account credentials, physical addresses and owned payment cards are stored in the \"supervisor\" app. You can access them via the APIs provided by the supervisor app.\n",
    "14. Once you have completed the task, call `apis.supervisor.complete_task()`. If the task asks for some information, return it as the answer argument, i.e. call `apis.supervisor.complete_task(answer=<answer>)`. For tasks that do not require an answer, just skip the answer argument or pass it as None.\n",
    "15. The answers, when given, should be just entity or number, not full sentences, e.g., `answer=10` for \"How many songs are in the Spotify queue?\". When an answer is a number, it should be in numbers, not in words, e.g., \"10\" and not \"ten\".\n",
    "16. You can also pass `status=\"fail\"` in the complete_task API if you are sure you cannot solve it and want to exit.\n",
    "17. You must make all decisions completely autonomously and not ask for any clarifications or confirmations from me or anyone else.\n",
    "\n",
    "USER:\n",
    "Using these APIs, now generate code to solve the actual task:\n",
    "\n",
    "My name is: {{ supervisor.first_name }} {{ supervisor.last_name }}. My personal email is {{ supervisor.email }} and phone number is {{ supervisor.phone_number }}.\n",
    "Task:\n",
    "\n",
    "{{ instruction }}\n",
    "\"\"\"\n",
    "ZERO_SHOT_PROMPT = \"\"\"\n",
    "USER:\n",
    "I am your supervisor and you are a super intelligent AI Assistant whose job is to achieve my day-to-day tasks completely autonomously.\n",
    "\n",
    "To do this, you will need to interact with app/s (e.g., spotify, venmo, etc) using their associated APIs on my behalf. For this you will undertake a *multi-step conversation* using a python REPL environment. That is, you will write the python code and the environment will execute it and show you the result, based on which, you will write python code for the next step and so on, until you've achieved the goal. This environment will let you interact with app/s using their associated APIs on my behalf.\n",
    "\n",
    "IMPORTANT CODE GENERATION RULES:\n",
    "1. Each code block must be completely self-contained - variables and functions from previous blocks are NOT available\n",
    "2. DO NOT reference or try to use variables or functions defined in previous code blocks\n",
    "3. Only generate valid Python code and comments\n",
    "4. DO NOT include conversation markers (USER:, ASSISTANT:) or any other text\n",
    "5. If you need data from a previous step, you must regenerate it or fetch it again\n",
    "6. Each code block starts fresh with no access to previous variables or functions\n",
    "\n",
    "\n",
    "Here are three key APIs that you need to know to get more information\n",
    "\n",
    "# To get a list of apps that are available to you.\n",
    "print(apis.api_docs.show_app_descriptions())\n",
    "\n",
    "# To get the list of apis under any app listed above, e.g. supervisor\n",
    "print(apis.api_docs.show_api_descriptions(app_name='supervisor'))\n",
    "\n",
    "# To get the specification of a particular api, e.g. supervisor app's show_account_passwords\n",
    "print(apis.api_docs.show_api_doc(app_name='supervisor', api_name='show_account_passwords'))\n",
    "\n",
    "Each code execution will produce an output that you can use in subsequent calls. Using these PROVIDED APIs, you can now generate code, that the environment will execute, to solve the task.\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "USER:\n",
    "**Key instructions and disclaimers**:\n",
    "\n",
    "1. The email addresses, access tokens and variables (e.g. spotify_password) in the example above were only for demonstration. Obtain the correct information by calling relevant APIs yourself.\n",
    "2. Only generate valid code blocks, i.e., do not put them in ```...``` or add any extra formatting. Any thoughts should be put as code comments.\n",
    "3. You can use the variables from the previous code blocks in the subsequent code blocks.\n",
    "4. Write small chunks of code and only one chunk of code in every step. Make sure everything is working correctly before making any irreversible change.\n",
    "5. The provided Python environment has access to its standard library. But modules and functions that have a risk of affecting the underlying OS, file system or process are disabled. You will get an error if do call them.\n",
    "6. Any reference to a file system in the task instructions means the file system *app*, operable via given APIs, and not the actual file system the code is running on. So do not write code making calls to os-level modules and functions.\n",
    "7. To interact with apps, only use the provided APIs, and not the corresponding Python packages. E.g., do NOT use `spotipy` for Spotify. Remember, the environment only has the standard library.\n",
    "8. The provided API documentation has both the input arguments and the output JSON schemas. All calls to APIs and parsing its outputs must be as per this documentation.\n",
    "9. For APIs that return results in \"pages\", make sure to consider all pages.\n",
    "10. To obtain current data or time, use Python functions like `datetime.now()` or obtain it from the phone app. Do not rely on your existing knowledge of what the current date or time is.\n",
    "11. For all temporal requests, use proper time boundaries, e.g., if I ask for something that happened yesterday, make sure to consider the time between 00:00:00 and 23:59:59. All requests are concerning a single, default (no) time zone.\n",
    "12. Any reference to my friends, family or any other person or relation refers to the people in my phone's contacts list.\n",
    "13. All my personal information, and information about my app account credentials, physical addresses and owned payment cards are stored in the \"supervisor\" app. You can access them via the APIs provided by the supervisor app.\n",
    "14. Once you have completed the task, call `apis.supervisor.complete_task()`. If the task asks for some information, return it as the answer argument, i.e. call `apis.supervisor.complete_task(answer=<answer>)`. For tasks that do not require an answer, just skip the answer argument or pass it as None.\n",
    "15. The answers, when given, should be just entity or number, not full sentences, e.g., `answer=10` for \"How many songs are in the Spotify queue?\". When an answer is a number, it should be in numbers, not in words, e.g., \"10\" and not \"ten\".\n",
    "16. You can also pass `status=\"fail\"` in the complete_task API if you are sure you cannot solve it and want to exit.\n",
    "17. You must make all decisions completely autonomously and not ask for any clarifications or confirmations from me or anyone else.\n",
    "\n",
    "USER:\n",
    "Using these APIs, now generate code to solve the actual task:\n",
    "\n",
    "My name is: {{ supervisor.first_name }} {{ supervisor.last_name }}. My personal email is {{ supervisor.email }} and phone number is {{ supervisor.phone_number }}.\n",
    "Task:\n",
    "\n",
    "{{ instruction }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = os.getcwd() + \"/models\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "model = model.to(\"cuda\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    device=\"cuda\",\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(messages: List[Dict[str, str]], pipe: pipeline) -> str:\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"].upper()\n",
    "        content = msg[\"content\"]\n",
    "        prompt += f\"{role}:\\n{content}\\n\\n\"\n",
    "    \n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            outputs = pipe(\n",
    "                prompt, \n",
    "                temperature=0.3,\n",
    "                max_new_tokens=512,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        response = outputs[0]['generated_text']\n",
    "        response = response[len(prompt):].strip()\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Generation failed: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReActAgent is modified from AppWorld's ReActAgent\n",
    "class ReActAgent:\n",
    "    def __init__(self, task: Task, prompt_type: str):\n",
    "        self.task = task\n",
    "        self.prompt_type = prompt_type\n",
    "        self.history: list[dict] = self.prompt_messages()\n",
    "\n",
    "    def prompt_messages(self) -> list[dict]:\n",
    "        dictionary = {\"supervisor\": self.task.supervisor, \"instruction\": self.task.instruction}\n",
    "        if self.prompt_type == \"one_shot\":\n",
    "            prompt = Template(ONE_SHOT_PROMPT.lstrip()).render(dictionary)\n",
    "        elif self.prompt_type == \"zero_shot\":\n",
    "            prompt = Template(ZERO_SHOT_PROMPT.lstrip()).render(dictionary)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid prompt type: {self.prompt_type}\")\n",
    "        messages: list[dict] = []\n",
    "        last_start = 0\n",
    "        for match in re.finditer(\"(USER|ASSISTANT|SYSTEM):\\n\", prompt):\n",
    "            last_end = match.span()[0]\n",
    "            if len(messages) == 0:\n",
    "                if last_end != 0:\n",
    "                    raise ValueError(\n",
    "                        f\"Start of the prompt has no assigned role: {prompt[:last_end]}\"\n",
    "                    )\n",
    "            else:\n",
    "                messages[-1][\"content\"] = prompt[last_start:last_end]\n",
    "            mesg_type = match.group(1).lower()\n",
    "            messages.append({\"role\": mesg_type, \"content\": None})\n",
    "            last_start = match.span()[1]\n",
    "        messages[-1][\"content\"] = prompt[last_start:]\n",
    "        return messages\n",
    "    \n",
    "    def next_code_block(self, pipe, last_execution_output: str | None = None) -> str:\n",
    "        if last_execution_output is not None:\n",
    "            self.history.append({\"role\": \"user\", \"content\": last_execution_output})\n",
    "        code = call_llm(self.history, pipe)\n",
    "        code = code.replace(\"ASSISTANT:\", \"\")\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": code})\n",
    "        return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    name: str\n",
    "    prompt_type: str\n",
    "    model: AutoModelForCausalLM\n",
    "    tokenizer: AutoTokenizer\n",
    "    max_interactions: int = 10\n",
    "    dataset_name: str = \"train\"\n",
    "    num_tasks: Optional[int] = None\n",
    "    output_dir: str = \"experiment_results\"\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        self.results = []\n",
    "        self.stats = {}\n",
    "        Path(config.output_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "    def setup_model(self):\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.config.model,\n",
    "            tokenizer=self.config.tokenizer,\n",
    "            pad_token_id=self.config.tokenizer.eos_token_id,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "        return pipe, self.config.tokenizer\n",
    "\n",
    "    def run_experiment(self) -> Dict:\n",
    "        pipe, tokenizer = self.setup_model()\n",
    "        task_ids = load_task_ids(self.config.dataset_name)\n",
    "        if self.config.num_tasks:\n",
    "            task_ids = task_ids[:self.config.num_tasks]\n",
    "        for index, task_id in enumerate(task_ids):\n",
    "            with AppWorld(task_id=task_id, experiment_name=self.config.name) as world:\n",
    "                result = self._run_single_task(world, pipe, tokenizer, task_id, index)\n",
    "                self.results.append(result)\n",
    "        self.stats = self._calculate_statistics()\n",
    "        self._save_results()\n",
    "        return self.stats\n",
    "\n",
    "    def _run_single_task(self, world: AppWorld, pipe, tokenizer, task_id, index) -> Dict:\n",
    "        agent = ReActAgent(task=world.task, prompt_type=self.config.prompt_type)  \n",
    "        output = None\n",
    "        iterations = []\n",
    "        total_tokens = 0\n",
    "        print(\"\\n\\n\" + \"*\" * 20 + f\" Task {index+1}/{self.config.num_tasks} ({task_id})  \" + \"*\" * 20)\n",
    "        for i in range(self.config.max_interactions):\n",
    "            code = agent.next_code_block(pipe, output)\n",
    "            tokens = len(tokenizer.encode(code))\n",
    "            total_tokens += tokens\n",
    "            output = world.execute(code)\n",
    "            if \"USER:\" in code:\n",
    "                # Added due to high frequency of LLM hallucinating USER output in the code.\n",
    "                output = \"ERROR: DO NOT SIMULATE USER, YOU ARE ASSISTANT. YOU MUST ONLY GENERATE VALID PYTHON CODE. DO NOT SIMULATE THE OUTPUT OF YOUR PYTHON CODE.\"                \n",
    "            iterations.append({\n",
    "                \"iteration\": i,\n",
    "                \"code\": code,\n",
    "                \"output\": output,\n",
    "                \"tokens\": tokens,\n",
    "                \"completed\": world.task_completed()\n",
    "            })\n",
    "            \n",
    "            if world.task_completed():\n",
    "                break\n",
    "                \n",
    "        return {\n",
    "            \"task_id\": task_id,\n",
    "            \"iterations\": iterations,\n",
    "            \"completed\": world.task_completed(),\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"num_iterations\": len(iterations)\n",
    "        }\n",
    "\n",
    "    def _calculate_statistics(self) -> Dict:\n",
    "        completed_tasks = [r for r in self.results if r[\"completed\"]]\n",
    "        all_iterations = []\n",
    "        for r in self.results:\n",
    "            all_iterations.extend(r[\"iterations\"])\n",
    "        error_iterations = [i for i in all_iterations if \"INVALID\" in i.get(\"output\", \"\")]\n",
    "        stats = {\n",
    "            \"total_tasks\": len(self.results),\n",
    "            \"completed_tasks\": len(completed_tasks),\n",
    "            \"completion_rate\": len(completed_tasks) / len(self.results) if self.results else 0,\n",
    "            \"avg_iterations\": np.mean([r[\"num_iterations\"] for r in self.results]) if self.results else 0,\n",
    "            \"error_rate\": len(error_iterations) / len(all_iterations) if all_iterations else 0,\n",
    "            \"avg_tokens_per_task\": np.mean([r[\"total_tokens\"] for r in self.results]) if self.results else 0,\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def _save_results(self):\n",
    "        output_dir = Path(self.config.output_dir) / self.config.name\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        with open(output_dir / \"results.json\", \"w\") as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        with open(output_dir / \"stats.json\", \"w\") as f:\n",
    "            json.dump(self.stats, f, indent=2)\n",
    "\n",
    "    def generate_plots(self):\n",
    "        output_dir = Path(self.config.output_dir) / self.config.name        \n",
    "        plt.bar([\"Completed\", \"Failed\"], \n",
    "                [self.stats[\"completed_tasks\"], \n",
    "                 self.stats[\"total_tasks\"] - self.stats[\"completed_tasks\"]])\n",
    "        plt.title(\"Task Completion Rate\")\n",
    "        plt.savefig(output_dir / \"completion_rate.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        iterations = [r[\"num_iterations\"] for r in self.results]\n",
    "        sns.histplot(iterations)\n",
    "        plt.title(\"Distribution of Iterations per Task\")\n",
    "        plt.xlabel(\"Number of Iterations\")\n",
    "        plt.savefig(output_dir / \"iterations_dist.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        tokens = [r[\"total_tokens\"] for r in self.results]\n",
    "        sns.histplot(tokens)\n",
    "        plt.title(\"Distribution of Tokens per Task\")\n",
    "        plt.xlabel(\"Number of Tokens\")\n",
    "        plt.savefig(output_dir / \"tokens_dist.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = os.getcwd() + \"/models\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/agentlm-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"THUDM/agentlm-7b\")\n",
    "\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENTLM BENCHMARK\n",
    "save_directory = os.getcwd() + \"/models\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/agentlm-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"THUDM/agentlm-7b\", device_map=\"auto\", torch_dtype=torch.bfloat16,low_cpu_mem_usage=True)\n",
    "\n",
    "config = ExperimentConfig(\n",
    "        name=\"agentlm-7b_one_shot\",\n",
    "        prompt_type=\"one_shot\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_interactions=10,\n",
    "        dataset_name=\"test_normal\",\n",
    "        num_tasks=15\n",
    "    )\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE-SHOT BENCHMARK\n",
    "save_directory = os.getcwd() + \"/models\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16,low_cpu_mem_usage=True)\n",
    "\n",
    "config = ExperimentConfig(\n",
    "        name=\"Llama-3.1-8B-ReAct-OneShot\",\n",
    "        prompt_type=\"one_shot\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_interactions=10,\n",
    "        dataset_name=\"test_normal\",\n",
    "        num_tasks=15\n",
    "    )\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZERO-SHOT BENCHMARK\n",
    "save_directory = os.getcwd() + \"/models\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16,low_cpu_mem_usage=True)\n",
    "\n",
    "config = ExperimentConfig(\n",
    "        name=\"Llama-3.1-8B-ReAct-ZeroShot\",\n",
    "        prompt_type=\"zero_shot\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_interactions=10,\n",
    "        dataset_name=\"test_normal\",\n",
    "        num_tasks=15\n",
    "    )\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner(config)\n",
    "stats = runner.run_experiment()\n",
    "\n",
    "runner.generate_plots()\n",
    "\n",
    "print(\"\\nExperiment Statistics:\")\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!appworld evaluate agentlm-7b_one_shot test_normal\n",
    "!appworld evaluate Llama-3.1-8B-ReAct-OneShot test_normal\n",
    "!appworld evaluate Llama-3.1-8B-ReAct-ZeroShot test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ReActAgent with Together API for synthetic data generation\n",
    "def call_together_api(prompt: str, api_key: str, model_name: str, \n",
    "                          temperature: float = 0.3, max_tokens: int = 512, top_p: float = 0.9) -> str:\n",
    "    time.sleep(1)\n",
    "    with tempfile.NamedTemporaryFile(mode='w+', delete=False) as temp:\n",
    "        request_body = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"top_p\": top_p,\n",
    "            # Added to stop the LLM from hallucinating USER output in the code.\n",
    "            \"stop\": [\"USER:\", \"USER\\n\"]\n",
    "        }\n",
    "        json.dump(request_body, temp)\n",
    "        temp_name = temp.name\n",
    "    \n",
    "    request = [\n",
    "        \"curl\", \"-s\", \"-X\", \"POST\", \"https://api.together.xyz/v1/completions\",\n",
    "        \"-H\", f\"Authorization: Bearer {api_key}\",\n",
    "        \"-H\", \"Content-Type: application/json\",\n",
    "        \"-d\", f\"@{temp_name}\"\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(request, capture_output=True, text=True, check=True)\n",
    "    response_json = json.loads(result.stdout)\n",
    "    os.unlink(temp_name)\n",
    "    generated_text = response_json.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "    if \"USER:\" in generated_text:\n",
    "        generated_text = generated_text.split(\"USER:\")[0].strip()\n",
    "        \n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def format(messages: List[Dict[str, str]]) -> str:\n",
    "    prompt = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"].upper()\n",
    "        content = msg[\"content\"]\n",
    "        prompt += f\"{role}:\\n{content}\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "class ReActAgent:\n",
    "    def __init__(self, task: Task, prompt_type: str):\n",
    "        self.task = task\n",
    "        self.prompt_type = prompt_type\n",
    "        self.history: list[dict] = self.prompt_messages()\n",
    "\n",
    "    def prompt_messages(self) -> list[dict]:\n",
    "        dictionary = {\"supervisor\": self.task.supervisor, \"instruction\": self.task.instruction}\n",
    "        if self.prompt_type == \"one_shot\":\n",
    "            prompt = Template(ONE_SHOT_PROMPT.lstrip()).render(dictionary)\n",
    "        elif self.prompt_type == \"zero_shot\":\n",
    "            prompt = Template(ZERO_SHOT_PROMPT.lstrip()).render(dictionary)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid prompt type: {self.prompt_type}\")\n",
    "        messages: list[dict] = []\n",
    "        last_start = 0\n",
    "        for match in re.finditer(\"(USER|ASSISTANT|SYSTEM):\\n\", prompt):\n",
    "            last_end = match.span()[0]\n",
    "            if len(messages) == 0:\n",
    "                if last_end != 0:\n",
    "                    raise ValueError(\n",
    "                        f\"Start of the prompt has no assigned role: {prompt[:last_end]}\"\n",
    "                    )\n",
    "            else:\n",
    "                messages[-1][\"content\"] = prompt[last_start:last_end]\n",
    "            mesg_type = match.group(1).lower()\n",
    "            messages.append({\"role\": mesg_type, \"content\": None})\n",
    "            last_start = match.span()[1]\n",
    "        messages[-1][\"content\"] = prompt[last_start:]\n",
    "        return messages\n",
    "    \n",
    "    def next_code_block(self, api_key: str, model_name: str, last_execution_output: str | None = None) -> str:\n",
    "        if last_execution_output is not None:\n",
    "            self.history.append({\"role\": \"user\", \"content\": last_execution_output})\n",
    "        prompt = format(self.history)\n",
    "        code = call_together_api(prompt, api_key, model_name)\n",
    "        code = code.replace(\"ASSISTANT:\", \"\")      \n",
    "        self.history.append({\"role\": \"assistant\", \"content\": code})\n",
    "        return code\n",
    "\n",
    "def generate_synthetic_data(api_key: str, model_name: str, \n",
    "                           dataset_name: str, max_interactions: int, num_tasks: Optional[int] = None, output_dir: str = \"synthetic_data\"):\n",
    "   \n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    task_ids = load_task_ids(dataset_name)\n",
    "    if num_tasks:\n",
    "        task_ids = task_ids[:num_tasks]\n",
    "    all_results = []\n",
    "    peft_data = []\n",
    "    \n",
    "    for _, task_id in enumerate(tqdm(task_ids, desc=\"Generating synthetic data\")):\n",
    "        with AppWorld(task_id=task_id, experiment_name=\"synthetic_data_gen\") as world:\n",
    "            agent = ReActAgent(task=world.task, prompt_type=\"one_shot\")  # Adjust prompt type as needed\n",
    "            output = None\n",
    "            iterations = []\n",
    "            for i in range(max_interactions):\n",
    "                print(f\"Iteration: {i}\")\n",
    "                code = agent.next_code_block(api_key, model_name, output)\n",
    "                \n",
    "                if \"USER:\" in code:\n",
    "                    output = \"ERROR: DO NOT SIMULATE USER, YOU ARE ASSISTANT. YOU MUST ONLY GENERATE VALID PYTHON CODE.\"\n",
    "                else:\n",
    "                    output = world.execute(code)\n",
    "                iterations.append({\n",
    "                    \"iteration\": i,\n",
    "                    \"code\": code,\n",
    "                    \"output\": output,\n",
    "                    \"completed\": world.task_completed()\n",
    "                })\n",
    "                if world.task_completed():\n",
    "                    break\n",
    "\n",
    "            result = {\n",
    "                \"task_id\": task_id,\n",
    "                \"instruction\": world.task.instruction,\n",
    "                \"iterations\": iterations,\n",
    "                \"completed\": world.task_completed()\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            \n",
    "            context = f\"USER: {world.task.instruction}\\n\\n\"\n",
    "            for i, iteration in enumerate(iterations):\n",
    "                if i > 0:\n",
    "                    prev_output = iterations[i-1][\"output\"]\n",
    "                    context += f\"USER: {prev_output}\\n\\n\"         \n",
    "                code = iteration[\"code\"]\n",
    "                peft_data.append({\n",
    "                    \"context\": context,\n",
    "                    \"response\": code,\n",
    "                    \"task_id\": task_id,\n",
    "                    \"turn\": i + 1,\n",
    "                    \"completed\": world.task_completed()\n",
    "                })\n",
    "                \n",
    "                context += f\"ASSISTANT: {code}\\n\\n\"\n",
    "            \n",
    "            save_results(all_results, output_dir)\n",
    "    save_results(all_results, output_dir)\n",
    "    print(f\"Saved {len(all_results)} tasks with {len(peft_data)} examples to {output_dir}\")\n",
    "    return all_results, peft_data\n",
    "\n",
    "def save_results(all_results, output_dir):\n",
    "    with open(os.path.join(output_dir, \"synthetic_data.json\"), \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train) Multiple runs to achieve larger N\n",
    "generate_synthetic_data(\n",
    "    api_key=\"06c36d1a1df03731104af386a4b91e5d6eefd819b1c6c1b50b2ddaecd7acdfbe\",\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    dataset_name=\"train\",\n",
    "    max_interactions=20,\n",
    "    output_dir=\"synthetic_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (dev) Multiple runs to achieve larger N\n",
    "generate_synthetic_data(\n",
    "    api_key=\"06c36d1a1df03731104af386a4b91e5d6eefd819b1c6c1b50b2ddaecd7acdfbe\",\n",
    "    model_name=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    dataset_name=\"dev\",\n",
    "    max_interactions=20,\n",
    "    output_dir=\"synthetic_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"fine_tuning_dataset.json\"\n",
    "OUTPUT_DIR = \"./llama-finetuned\"\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 4\n",
    "USE_4BIT = True\n",
    "\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    all_examples = json.load(f)\n",
    "    \n",
    "dataset = Dataset.from_list(all_examples)\n",
    "dataset_splits = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset_splits[\"train\"]\n",
    "eval_dataset = dataset_splits[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(examples):\n",
    "    texts = [\n",
    "        f\"User: {instruction}\\n\\nAssistant: {response}\" \n",
    "        for instruction, response in zip(examples[\"instruction\"], examples[\"response\"])\n",
    "    ]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"response\"]\n",
    ")\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"response\"]\n",
    ")\n",
    "print(\"Tokenization Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16,low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"input_ids\", \"attention_mask\"]\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING STARTED\")\n",
    "trainer.train()   \n",
    "print(f\"Saving final model to {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_directory = \"/home/ubuntu/nl-/nlp_final_project/Code/llama-finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "peft_config = PeftConfig.from_pretrained(save_directory)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, save_directory)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "config = ExperimentConfig(\n",
    "        name=\"Llama-3.1-8B-Finetuned-ReAct-ZeroShot\",\n",
    "        prompt_type=\"zero_shot\",\n",
    "        model=merged_model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_interactions=10,\n",
    "        dataset_name=\"test_normal\",\n",
    "        num_tasks=15\n",
    "    )\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = ExperimentRunner(config)\n",
    "stats = runner.run_experiment()\n",
    "\n",
    "runner.generate_plots()\n",
    "\n",
    "print(\"\\nExperiment Statistics:\")\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- https://huggingface.co/docs (Used for training, loading models, inference, etc.)\n",
    "- https://appworld.dev/ (Used for prompts and ReAct Agent Framework)\n",
    "- Claude 3.5 (Used to help with debugging, File Reading/Writing, and chart/graph generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
